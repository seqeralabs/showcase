{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Seqera Platform: Demonstration Walkthrough","text":""},{"location":"#overview","title":"Overview","text":"<p>This guide provides a walkthrough of a standard Seqera Platform demonstration. The demonstration will describe how to add and run a pipeline in the Platform, examine the run details, and highlight key features such as pipeline optimization, Data Explorer, and Data Studios.</p> <p>The demonstration will focus on using the nf-core/rnaseq pipeline as an example to execute a Nextflow pipeline on Seqera Cloud via the AWS Batch cloud executor.</p>"},{"location":"#requirements","title":"Requirements","text":"<p> Access to Seqera Platform  Seqera Main Site</p> <p> Seqera Cloud account</p> <p> Access to a workspace in Seqera Cloud</p> <p>  Access to an AWS Batch compute environment created in that workspace</p> <p> Publicly available nf-core/rnaseq pipeline repository</p> <p> Input samplesheet to run the nf-core/rnaseq pipeline on Seqera Cloud</p>"},{"location":"001_what_is_nextflow/","title":"What is Nextflow?","text":""},{"location":"001_what_is_nextflow/#introduction-to-nextflow","title":"Introduction to Nextflow","text":"<p>Nextflow is a domain-specific language (DSL) for enabling scalable, portable, and reproducible workflows.</p> <p>Nextflow is both a workflow language and an execution runtime that supports a wide range of execution platforms, including popular traditional grid scheduling systems such as Slurm and IBM LSF, and cloud services such as AWS Batch, Google Cloud Batch, Azure Batch and Kubernetes.</p> <p>While Nextflow solves many of the technical challenges associated with building and executing data pipelines; bioinformaticians, data scientists, and clinicians still face challenges:</p> <ul> <li>Users shouldn't need extensive technical knowledge of the command line or have to understand how to configure computing environments to monitor pipeline execution.</li> <li>Researchers face difficulties in ensuring reproducibility, tracking data provenance, and sharing comprehensive reports and interactive tools for data analysis.</li> </ul>"},{"location":"001_what_is_nextflow/#limitations","title":"Limitations","text":"<p>Monitoring and launching workflows via the Nextflow CLI, though direct, poses challenges, especially with complex or large-scale pipelines that are not as simple as running a Hello World pipeline:</p> <ul> <li>Scalability: As the number of tasks increases, manually checking individual log files becomes impractical.</li> <li>Real-time tracking: The CLI does not offer an easy way to visualize real-time progress across multiple parallel tasks.</li> <li>Aggregation: Collecting and interpreting logs from various processes requires additional tools or scripts, complicating the workflow management.</li> <li>Flexibility: Switching between environments (i.e., your local computer to HPC or cloud) requires the setup of access in the form of account keys and credentials to the environment on your CLI, followed by using the appropriate Nextflow configuration settings.</li> </ul>"},{"location":"002_what_is_the_seqera_platform/","title":"What is the Seqera Platform?","text":""},{"location":"002_what_is_the_seqera_platform/#introduction-to-the-seqera-platform","title":"Introduction to the Seqera Platform","text":"<p>The Seqera Platform is an intuitive centralized command post designed to make scientific analysis accessible at any scale. </p> <p>The Platform acts as a pane of glass, enabling users to effortlessly launch, manage, monitor, and collaborate on scalable Nextflow data analysis using their own computing resources and infrastructure. Researchers can focus on the science that matters rather than worrying about infrastructure engineering.</p> <p>The Seqera Platform helps organizations:</p> <ul> <li>Launch, manage, and monitor portable Nextflow pipelines from anywhere in real-time</li> <li>Enable non-technical users to run pipelines via the intuitive Launchpad interface</li> <li>Easily provision and leverage cloud-based compute environments</li> <li>Collaborate and share pipelines and data securely among local and remote teams</li> <li>Easily access libraries of production-proven Nextflow community pipelines available from nf-core</li> <li>Automate complex tasks as part of broader enterprise processes</li> </ul>"},{"location":"002_what_is_the_seqera_platform/#deployment-methods","title":"Deployment methods","text":"<p>Seqera offers two deployment methods:</p> <ul> <li>Seqera Cloud: A version of the application available as a SaaS solution, hosted on Seqera's infrastructure.</li> <li>Seqera Enterprise: A lightweight, deployable version of the application that can be hosted on our customers' infrastructure.</li> </ul>"},{"location":"002_what_is_the_seqera_platform/#core-components","title":"Core components","text":"<p>The Platform consists of three main architectural components: a backend container, a frontend container, and a database that stores all of the data required by the application. The frontend container communicates with the backend container and database via API calls. As a result, all features and activities available through the user interface can also be accessed programmatically via the Seqera Platform API. For more information, see the Automation section later in the walkthrough.</p> <p>This walkthrough will demonstrate the various features of the Seqera Platform which makes it easier to build, launch, and manage scalable data pipelines.</p>"},{"location":"003_accessing_the_platform/","title":"Access the Platform","text":""},{"location":"003_accessing_the_platform/#login","title":"Login","text":"<p>Log in to Seqera Cloud, either through a GitHub or Google account, or by providing an email address. If you are signing in for the first time, Seqera Cloud will send an authentication link to the email address, enabling you to login.</p> Click to show animation <p></p>"},{"location":"003_accessing_the_platform/#organizations-and-workspaces","title":"Organizations and workspaces","text":"<p>All resources in the Seqera Platform are managed within organizations, typically named the same as your organization, such as Seqera or Johnson and Johnson. An organization can contain multiple workspaces. Each workspace is an isolated environment that can consist of different users, pipelines, credentials and compute infrastructure. Workspaces enable user access to the Platform depending on the organizational needs. Typically, teams of colleagues or collaborators will have access to one or more workspaces, and all resources in a workspace (pipelines, compute environments, datasets, etc.) are shared by members of that workspace.</p> <p>With workspaces, organization owners can:</p> <ul> <li>Help research teams segment their work depending on their needs.</li> <li>Enable teams to focus on specific activities, such as core R&amp;D or clinical trials with PII, which need to be kept secure.</li> <li>Create workspaces for specific internal departments like oncology, neuroscience, or therapeutics.</li> <li>Add organization users to workspaces as teams or groups.</li> </ul> <p>Navigate to the <code>seqeralabs/showcase</code> workspace which contains all of the relevant entities required for this walkthrough guide.</p> Click to show animation <p></p>"},{"location":"004_launching_pipelines/","title":"Launch pipelines","text":""},{"location":"004_launching_pipelines/#launchpad","title":"Launchpad","text":"<p>Each workspace has a Launchpad that allows users to easily create and share Nextflow pipelines that can be executed on any infrastructure supported by the Platform, including all public clouds and most HPC schedulers. A Launchpad pipeline consists of a pre-configured workflow repository, compute environment, and launch parameters.</p> <p>Users can create their own pipelines, share them with others on the Launchpad, or tap into over a hundred community pipelines available on nf-core and other sources.</p> Advanced <p>Adding a new pipeline is relatively simple and can be included as part of the demonstration. See Add a Pipeline.</p>"},{"location":"004_launching_pipelines/#launch-the-nf-corernaseq-pipeline","title":"Launch the nf-core/rnaseq pipeline","text":""},{"location":"004_launching_pipelines/#1-go-to-launchpad","title":"1. Go to Launchpad","text":"<p>Navigate to the Launchpad in the <code>seqeralabs/showcase</code> workspace and select Launch next to the <code>nf-core-rnaseq</code> pipeline to open the launch form.</p> Click to show animation <p></p>"},{"location":"004_launching_pipelines/#2-nextflow-parameter-schema","title":"2. Nextflow parameter schema","text":"<p>When you select Launch, a parameters page is shown to allow you to fine-tune the pipeline execution. This parameters form is rendered from a file called <code>nextflow_schema.json</code> which can be found in the root of the pipeline Git repository. The <code>nextflow_schema.json</code> file is a simple JSON-based schema describing pipeline parameters that allows pipeline developers to easily adapt their in-house Nextflow pipelines to be executed via the interactive Seqera Platform web interface.</p> <p>See the \"Best Practices for Deploying Pipelines with the Seqera Platform\" blog for further information on how to automatically build the parameter schema for any Nextflow pipeline using tooling maintained by the nf-core community. </p>"},{"location":"004_launching_pipelines/#3-parameter-selection","title":"3. Parameter selection","text":"<p>Adjust the following Platform-specific options if needed:</p> <ul> <li> <p><code>Workflow run name</code>:</p> <p>A unique identifier for the run, pre-filled with a random name. This can be customized.</p> </li> <li> <p><code>Labels</code>:</p> <p>Assign new or existing labels to the run. For example, Project ID or genome version.</p> </li> </ul> <p>Each pipeline including nf-core/rnaseq will have its own set of parameters that need to be provided in order to run it. The following parameters are mandatory:</p> <ul> <li> <p><code>input</code>:</p> <p>Most nf-core pipelines have standardized the usage of the <code>input</code> parameter to specify an input samplesheet that contains paths to any input files (such as FastQ files) and any additional metadata required to run the pipeline. The <code>input</code> parameter can accept a file path to a samplesheet in the S3 bucket selected through Data Explorer (such as <code>s3://my-bucket/my-samplesheet.csv</code>). Alternatively, the Seqera Platform has a Datasets feature that allows you to upload structured data like samplesheets for use with Nextflow pipelines.</p> <p>For the purposes of this demonstration, select Browse next to the <code>input</code> parameter and search and select a pre-loaded dataset called \"rnaseq_samples\".</p> Click to show animation <p></p> Advanced <p>Users can upload their own samplesheets and make them available as a dataset in the 'Datasets' tab. See Add a dataset.</p> </li> <li> <p><code>outdir</code>:</p> <p>Most nf-core pipelines have standardized the usage of the <code>outdir</code> parameter to specify where the final results created by the pipeline are published. <code>outdir</code> must be different for each different pipeline run. Otherwise, your results will be overwritten. Since we want to publish these files to an S3 bucket, we must provide the directory path to the appropriate storage location (such as `s3://my-bucket/my-results).</p> <p>For the <code>outdir</code> parameter, specify an S3 directory path manually, or select Browse to specify a cloud storage directory using Data Explorer.</p> Click to show animation <p></p> </li> </ul> <p>Users can easily modify and specify other parameters to customize the pipeline execution through the parameters form. For example, in the Read trimming options section of the parameters page, change the <code>trimmer</code> to select <code>fastp</code> in the dropdown menu, instead of <code>trimgalore</code>, and select Launch button.</p> <p></p>"},{"location":"005_adding_a_pipeline/","title":"Add a pipeline to the Launchpad","text":"<p>The Launchpad allows you to create a preconfigured set of Nextflow pipelines that are ready to be executed on any compute environment in a given workspace. This allows users to launch pipelines and customize the appropriate pipeline-level parameters without the need to understand the complexities of the underlying compute infrastructure.</p>"},{"location":"005_adding_a_pipeline/#adding-the-nf-corernaseq-pipeline","title":"Adding the nf-core/rnaseq pipeline","text":"<p>For this walkthrough, we will add the nf-core/rnaseq pipeline to the Launchpad.</p> <p>Select Add Pipeline and specify:</p> <ul> <li>Name: <code>nf-core-rnaseq-yeast</code></li> <li>Description: <code>nf-core/rnaseq pipeline configured for yeast data</code><ul> <li>(Optional) Free text summary of the pipeline that may be useful to users when selecting a pipeline to launch. </li> </ul> </li> <li>Labels: <code>yeast</code><ul> <li>(Optional) Labels allow you to categorize the pipeline according to arbitrary criteria (such as reference genome version) that may help users to select the appropriate pipeline for their analysis from the Launchpad.</li> </ul> </li> <li>Compute environment: <code>seqera_aws_ireland_fusionv2_nvme</code><ul> <li>Select an existing workspace compute environment. In this case, it is an AWS Batch compute environment in Ireland that has been pre-configured to use Fusion version 2.</li> </ul> </li> <li>Pipeline to launch: <code>https://github.com/nf-core/rnaseq</code><ul> <li>Platform allows you to select any public or private Git repository that contains Nextflow source code.</li> </ul> </li> <li>Revision number: <code>3.14.0</code><ul> <li>When you provide the Pipeline to launch, Platform will search all of the available tags and branches in the upstream pipeline repository and render a dropdown to select the appropriate version. Selecting a specific version is very important for reproducibility to ensure each run generates the same results.</li> </ul> </li> <li>Config profiles: <code>test</code><ul> <li>(Optional) Platform allows you to select a profile that has been defined in the Nextflow pipeline. All nf-core pipelines have a <code>test</code> profile that is associated with a minimal test dataset. This profile runs the pipeline with heavily sub-sampled input data for the purposes of CI/CD and to quickly confirm that the pipeline runs on any given infrastructure. The <code>test</code> profile of the nf-core/rnaseq pipeline was created using yeast data, which is why we add that particular annotation in the Labels section.</li> </ul> </li> <li>Pipeline parameters:<ul> <li>(Optional) You can set any custom pipeline parameters in this section that will be prepopulated when users launch the pipeline from the Launchpad. For example, set the path to local reference genomes so users don't have to worry about locating these files when launching the pipeline.</li> </ul> </li> <li>Pre-run script:<ul> <li>(Optional) You can define Bash code that executes before the pipeline launches in the same environment where Nextflow runs. Pre-run scripts are useful for defining executor settings, troubleshooting, and defining a specific version of Nextflow with the <code>NXF_VER</code> environment variable.</li> </ul> </li> </ul> <p>Once you have populated the appropriate settings, select Add and this pipeline will become available for other users in the same workspace to launch within the preconfigured compute infrastructure.</p> <p></p> <p></p> <p></p>"},{"location":"006_adding_a_dataset/","title":"Datasets","text":"<p>Most bioinformatics pipelines require an input of some sort. This is typically a samplesheet where each row consists of a sample, the location of files for that sample (such as fastq files), and other sample details.</p> <p>In the cloud, users may need to upload this samplesheet to a bucket or retrieve the path to this file on a shared filesystem to use it as pipeline input.</p> <p>Instead, these samplesheets can be made easily accessible through the Datasets feature in Platform. Datasets allow users to upload a structued CSV (Comma-Separated Values) or TSV (Tab-Separated Values) file to a workspace. They are then used as inputs to pipelines to simplify data management, minimize user data-input errors, and facilitate reproducible workflows.</p>"},{"location":"006_adding_a_dataset/#1-download-the-nf-corernaseq-test-samplesheet","title":"1. Download the nf-core/rnaseq test samplesheet","text":"<p>The nf-core/rnaseq pipeline works with input datasets (samplesheets) containing sample names, fastq file locations, and indications of strandedness. The Seqera Community Showcase sample dataset for nf-core/rnaseq looks like this:</p> <p>Example rnaseq dataset</p> <p> sample fastq_1 fastq_2 strandedness WT_REP1 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse WT_REP1 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse WT_REP2 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_UNINDUCED_REP1 s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_UNINDUCED_REP2 s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_UNINDUCED_REP2 s3://nf-core-awsmegatests/rnaseq/... reverse RAP1_IAA_30M_REP1 s3://nf-core-awsmegatests/rnaseq/... s3://nf-core-awsmegatests/rnaseq/... reverse <p></p> <p>Download the nf-core/rnaseq samplesheet_test.csv provided in this repository to your computer.</p>"},{"location":"006_adding_a_dataset/#2-add-the-dataset","title":"2. Add the Dataset","text":"<p>From the Datasets tab, select Add Dataset.</p> <p></p> <p>Specify the following dataset details:</p> <ul> <li>A Name for the dataset, such as 'nf-core-rnaseq-test-dataset'</li> <li>A Description</li> <li>Select the First row as header option</li> <li>Select Upload file and browse to the CSV file downloaded from this repository. This CSV file specifies the paths to 7 small FASTQ files for a sub-sampled Yeast RNAseq dataset.</li> </ul> <p>Notice the location of the files point to a path on S3. This could also be a path to a shared filesystem, if using an HPC compute environment. Using the path to the files, Nextflow will stage the files into the task working directory. </p> Note <p>Seqera Platform will not store any data used for analysis in pipelines. The datasets must include locations of data that is stored elsewhere, on the user's infrastructure.</p>"},{"location":"007_monitoring_runs/","title":"Monitor runs","text":"<p>There are several ways to monitor run executions in Seqera Platform.</p>"},{"location":"007_monitoring_runs/#1-workspace-view","title":"1. Workspace view","text":"<p>A full history of all pipeline executions within a given workspace can be accessed via the Runs tab:</p> Click to show animation <p></p>"},{"location":"007_monitoring_runs/#2-all-runs-view","title":"2. All runs view","text":"<p>Access the All runs page from the user menu in the top right of the interface. This page provides a comprehensive overview of the runs across the entire Platform instance. The default view will be all organizations and workspaces accessible to the user. However, you can select visible workspaces from the dropdown next to View, and filter for a particular set of runs using any of the following fields:</p> <ul> <li><code>status</code></li> <li><code>label</code></li> <li><code>workflowId</code></li> <li><code>runName</code></li> <li><code>username</code></li> <li><code>projectName</code></li> <li><code>after: YYYY-MM-DD</code></li> <li><code>before: YYYY-MM-DD</code></li> <li><code>sessionId</code></li> <li><code>is:starred</code></li> </ul> <p>For example:</p> <pre><code>rnaseq username:johndoe status:succeeded after:2024-01-01\n</code></pre> Click to show animation <p></p>"},{"location":"007_monitoring_runs/#3-dashboard-view","title":"3. Dashboard view","text":"<p>Access the Dashboard from the user menu in the top right of the interface. This page provides an overview of the total runs across the Platform instance that are currently submitted, running, or have failed. The default view will be all organizations and workspaces accessible to the user. However, you can select visible workspaces from the dropdown next to View, and filter by time, including a custom date range up to 12 months. You can also select Export data to download a CSV file with all the available export data.</p> Click to show animation <p></p>"},{"location":"008_viewing_run_information/","title":"Run Information","text":"<p>When you launch a pipeline, you are navigated to the Runs tab which contains all executed workflows, with your submitted run at the top of the list. </p> <p>Each new or resumed job is given a random name, such as \"grave_williams\". Each row corresponds to a specific job. As a job executes, it can transition through the following states:</p> <ul> <li>submitted: Pending execution</li> <li>running: Running</li> <li>succeeded: Completed successfully</li> <li>failed: Successfully executed, where at least one task failed with a terminate error strategy</li> <li>cancelled: Stopped forceably during execution</li> <li>unknown: Indeterminate status</li> </ul> Click to show animation <p></p>"},{"location":"008_viewing_run_information/#1-view-the-run-details-for-nf-corernaseq","title":"1. View the run details for nf-core/rnaseq","text":"<p>The pipeline launched in the previous step will take some time to begin running. In the meantime, we can take a look at a previous successful run to observe the run details.</p> <p>Select a previous run to view the Run details page.</p>"},{"location":"008_viewing_run_information/#2-run-details-page","title":"2. Run details page","text":"<p>As the pipeline begins to run, you will see the run details populating with the following tabs:</p> <ul> <li> <p>Command-line: The Nextflow command invocation that would be used to run the pipeline. This contains details about the version (through the <code>-r</code> flag), and profile, if specified (through the <code>-profile</code> flag).</p> </li> <li> <p>Parameters: The exact set of parameters used in the execution. This is helpful for reproducing results of a previous run.</p> </li> <li> <p>Resolved Nextflow configuration: The full Nextflow configuration settings used for the run. This includes parameters, but also settings specific to task execution (such as memory, CPUs, and output directory).</p> </li> <li> <p>Execution Log: A summarized Nextflow log providing information about the pipeline and the status of the run.</p> </li> <li> <p>Datasets: Link to datasets, if any were used in the run.</p> </li> <li> <p>Reports: View outputs of your pipeline directly in the Platform.</p> </li> </ul> Click to show animation <p></p>"},{"location":"008_viewing_run_information/#3-view-reports","title":"3. View reports","text":"<p>Most Nextflow pipelines will generate reports or output files which are useful to inspect at the end of the pipeline execution. </p> <p>Reports can contain quality control (QC) metrics that are important to assess the integrity of the results.</p> <p>For example, for the nf-core/rnaseq pipeline, you can view the MultiQC report generated. MultiQC is a helpful reporting tool to generate aggregate statistics and summaries from bioinformatics tools.</p> <p></p> <p></p> <p>Notice the path to the file will still point to a location on the user's infrastructure, but we can view the contents and download the file without having to go to the Cloud or a remote filesystem.</p>"},{"location":"008_viewing_run_information/#specifying-outputs-in-reports","title":"Specifying outputs in reports","text":"<p>To customize and instruct Platform where to find reports generated by the pipeline, a YAML file tower.yml that contains the locations of the generated reports must be included in the pipeline repository. </p> <p>In the nf-core/rnaseq pipeline, the MULTIQC process step generates a MultiQC report file in HTML format.</p> <pre><code>reports:\n  multiqc_report.html:\n    display: \"MultiQC HTML report\"\n</code></pre>"},{"location":"008_viewing_run_information/#4-view-general-information","title":"4. View general information","text":"<p>The run details page includes general information about who executed the run and when, the Git hash and tag used, and additional details about the compute environment and Nextflow version used.</p> Click to show animation <p></p> <p>The 'General' panel displays top-level information about a pipeline run:</p> <ul> <li>Unique workflow run ID</li> <li>Workflow run name</li> <li>Timestamp of pipeline start</li> <li>Project revision and Git commit ID</li> <li>Nextflow session ID</li> <li>Username of the launcher</li> <li>Work directory path</li> </ul>"},{"location":"008_viewing_run_information/#5-view-details-for-a-task","title":"5. View details for a Task","text":"<p>Scroll down the page and you will see:</p> <ul> <li>The progress of individual pipeline Processes</li> <li>Aggregated stats for the run (total walltime, CPU hours)</li> <li>Workflow metrics (CPU efficiency, memory efficiency)</li> <li>A Task details table for every task in the workflow</li> </ul> <p>The task details table can provide further information on every step in the pipeline, including task statuses and metrics.</p>"},{"location":"008_viewing_run_information/#6-task-details-window","title":"6. Task details window","text":"<p>Select a task in the task table to open the Task details dialog. The dialog has three tabs: About, Execution log and Data Explorer.</p> <p>About</p> <p>The About tab provides the following information:</p> <ol> <li> <p>Name: Process name and tag</p> </li> <li> <p>Command: Task script, defined in the pipeline process</p> </li> <li> <p>Status: Exit code, task status, number of attempts</p> </li> <li> <p>Work directory: Directory where the task was executed</p> </li> <li> <p>Environment: Environment variables that were supplied to the task</p> </li> <li> <p>Execution time: Metrics for task submission, start, and completion time</p> </li> <li> <p>Resources requested: Metrics for the resources requested by the task</p> </li> <li> <p>Resources used: Metrics for the resources used by the task</p> </li> </ol> Click to show animation <p></p> <p>Execution log</p> <p>The Execution log tab provides a real-time log of the selected task's execution. Task execution and other logs (such as stdout and stderr) are available for download from here, if still available in your compute environment.</p>"},{"location":"008_viewing_run_information/#7-task-work-directory-in-data-explorer","title":"7. Task work directory in Data Explorer","text":"<p>If a task fails, a good place to begin troubleshooting is the task's work directory.</p> <p>Nextflow hash-addresses each task of the pipeline and creates unique directories based on these hashes. Instead of navigating through a bucket on the Cloud console or filesystem to find the contents of this directory, we can make use of the 'Data Explorer' tab in the Task window.</p> <p>The Data Explorer tab allows you to view the log files and output files generated for each task in it's working directory, directly within Platform.</p> <p>You can view, download, and retrieve the link for these intermediate files stored in the Cloud from the Explorer tab, making troubleshooting much simpler.</p> Click to show animation <p></p>"},{"location":"008_viewing_run_information/#8-resume-a-pipeline","title":"8. Resume a Pipeline","text":"<p>Seqera Platform enables you to use Nextflow's resume functionality to resume a workflow run with the same parameters, using the cached results of previously completed tasks and only executing failed and pending tasks.</p> Click to show animation <p></p> <p>To resume a failed or cancelled run:</p> <ul> <li>Select the three dots next to the run</li> <li>Select Resume from the options menu</li> <li>Edit the parameters before launch, if needed</li> <li>If you have the appropriate permissions, you may edit the compute environment if needed</li> </ul>"},{"location":"009_optimizing_pipelines/","title":"Optimize a pipeline","text":""},{"location":"009_optimizing_pipelines/#resource-usage-and-pipeline-optimization","title":"Resource usage and pipeline optimization","text":"<p>The task-level resource usage metrics provided by Platform allow you to determine the resources requested for a task and what was actually used. This information helps you fine-tune your configuration more accurately.</p> <p>However, manually adjusting resources for every task in your pipeline is impractical. Instead, you can leverage the pipeline optimization feature available on the Launchpad.</p> <p>Seqera's pipeline optimization analyzes resource usage data from previous runs to optimize the resource allocation for future runs. After a successful run, optimization becomes available, indicated by the lightbulb icon next to the pipeline turning black.</p>"},{"location":"009_optimizing_pipelines/#1-optimize-nf-corernaseq","title":"1. Optimize nf-core/rnaseq","text":"<p>Navigate back to the Launchpad and select the lightbulb icon next to the nf-core/rnaseq pipeline to view the optimized profile. You have the flexibility to tailor the optimization's target settings and incorporate a retry strategy as needed.</p>"},{"location":"009_optimizing_pipelines/#2-view-optimized-configuration","title":"2. View optimized configuration","text":"<p>When you select the lightbulb, you can access an optimized configuration profile in the second tab of the Customize optimization profile window.</p> <p>This profile consists of Nextflow configuration settings for each process and each resource directive (where applicable):  cpus, memory, and time. The optimized setting for a given process and resource directive is based on the maximum use of that resource across all tasks in that process.</p> <p>Once optimization is selected, subsequent runs of that pipeline will inherit the optimized configuration profile, indicated by the black lightbulb icon with a checkmark. </p> <p>NOTE: Optimizated profiles are generated from one run at a time, defaulting to the most recent runs, and not an aggregation of previous runs.</p> Click to show animation <p></p> <p>You can verify the optimized configuration of a given run by inspecting the resource usage plots for that run and these fields in the run's task table:</p> Description Key CPU usage <code>pcpu</code> Memory usage <code>peakRss</code> Runtime <code>start</code> and <code>complete</code>"},{"location":"010_using_data_explorer/","title":"Data Explorer","text":"<p>When running pipelines in the Cloud, users typically need access to buckets and blob storage to view pipeline results and upload files (such as samplesheets and reference data) for analysis. Managing credentials and permissions for multiple users, as well as training users to navigate Cloud consoles and CLIs, can be complicated. Instead, users can view their data directly through Data Explorer.</p> <p>With Data Explorer, you can browse and interact with remote data repositories from organization workspaces in Seqera Platform. It supports AWS S3, Azure Blob Storage, and Google Cloud Storage repositories.</p>"},{"location":"010_using_data_explorer/#view-pipeline-outputs-in-data-explorer","title":"View pipeline outputs in Data Explorer","text":"<p>In Data Explorer, you can:</p> <ul> <li> <p>View bucket details:     View the cloud provider, bucket address, and credentials by selecting the information icon next to a bucket in the Data Explorer list.</p> Click to show animation <p></p> </li> <li> <p>View bucket contents     Select a bucket name from the Data Explorer list to view the contents of that bucket. </p> <p>The file type, size, and path of objects are displayed in columns to the right of the object name. For example, we can take a look at the outputs of our nf-core/rnaseq run.</p> Click to show animation <p></p> </li> <li> <p>Preview files:      Select a file to open a preview window that includes a Download button. For example, we can use Data Explorer to view the results of the nf-core/rnaseq pipeline that we executed. Specifically, we can view the resultant gene counts of the salmon quantification step:</p> Click to show animation <p></p> </li> </ul>"},{"location":"010_using_data_explorer/#configure-a-bucket-to-browser-in-data-explorer","title":"Configure a bucket to browser in Data Explorer","text":"<p>Data Explorer also enables you to add public cloud storage buckets to view and use data from resources such as:</p> <ul> <li>The Cancer Genome Atlas (TCGA)</li> <li>1000 Genomes Project</li> <li>NCBI SRA</li> <li>Genome in a Bottle Consortium</li> <li>MSSNG Database</li> <li>Genome Aggregation Database (gnomAD) </li> </ul>"},{"location":"010_using_data_explorer/#add-a-cloud-bucket","title":"Add a cloud bucket","text":"<p>Select Add cloud bucket from the Data Explorer tab to add individual buckets (or directory paths within buckets). </p>"},{"location":"010_using_data_explorer/#fill-in-bucket-details","title":"Fill in bucket details","text":"<p>Specify the Provider, Bucket path, Name, Credentials, and Description, then select Add. For public cloud buckets, select Public from the Credentials dropdown menu.</p> Click to show animation <p></p> <p>You can now use this data in your analysis without having to interact with Cloud consoles or CLI tools. </p>"},{"location":"011_tertiary_analysis_data_studios/","title":"Tertiary analysis in Data Studios","text":""},{"location":"011_tertiary_analysis_data_studios/#introduction-to-data-studios","title":"Introduction to Data Studios","text":"<p>After running a pipeline, you may want to perform tertiary analysis in platforms like Jupyter Notebook or RStudio. Setting up the infrastructure for these platforms, including accessing pipeline data, results, and necessary bioinformatics packages, can be complex and time-consuming.</p> <p>Data Studios streamlines this process for Seqera Platform users by allowing them to add interactive analysis environments based on templates, similar to how they add and share pipelines and datasets.</p> <p>Platform manages all the details, enabling users to easily select their preferred interactive tool and analyze their data within the platform.</p> <p>On the Data Studios tab, you can monitor and see the details of the data studios in your workspace.</p> <p>Data studios will have a name, followed by the cloud provider they are run on, the container image being used (Jupyter, VS Code, or RStudio), the user who created the data studio, the timestamp of creation, and the status indicating whether it has started, stopped, or is running. </p> <p></p> <p>Select the three dots menu to: - See the details of the data studio - Connect to the studio - Start the studio - Stop the studio - Copy the data studio URL</p>"},{"location":"011_tertiary_analysis_data_studios/#analyse-rnaseq-data-in-data-studios","title":"Analyse RNAseq data in Data Studios","text":"<p>We can use Data Studios to perform bespoke analysis on the results of upstream workflows. For example, we can run the nf-core/rnaseq workflow to quantify gene expression, followed by nf-core/differentialabundance to derive differential expression statistics, and then use Data Studios to interrogate and visualize the results of those analyses. </p> <p>As an example, we have run RNAseq results through the nf-core/differentialabundance pipeline and created a new data studio with these results from the cloud mounted into the studio to perform further analysis. One of these outputs is a Shiny application, which we can deploy for interactive analysis.</p> Presenter's Note <p>The Data Studios demonstration involves deploying an RShiny app to explore RNAseq results. This will open the app in a new browser window. You may choose to have this window already open before your demo and switch over to it.</p>"},{"location":"011_tertiary_analysis_data_studios/#1-open-the-rnaseq-analysis-studio","title":"1. Open the RNAseq analysis studio","text":"<p>Select the rnaseq_to_differentialabundance data studio.</p> <p>When selecting the studio, you will see we can create an RStudio environment that uses an existing compute environment available in this workspace. </p> <p>We have also mounted data generated from our RNAseq pipeline, and subsequent differentialabundance pipeline, directly from AWS S3. </p> <p>We can also specify the resources this studio will use. </p> Click to show animation <p></p>"},{"location":"011_tertiary_analysis_data_studios/#2-connect-to-the-data-studio","title":"2. Connect to the data studio","text":"<p>This data studio will start an RStudio environment in which we have already installed the necessary R packages for deploying an RShiny app that will allow us to interact with various comparisons and visualizations of our RNAseq data. We've also generated an R Markdown document with the commands in place to generate the RShiny application.</p> Click to show animation <p></p> <p>We can deploy the RShiny app in the data studio by selecting the green play button on the last \"chunk\" or section of the R script:</p> <p></p>"},{"location":"011_tertiary_analysis_data_studios/#3-explore-results-in-rshiny-app","title":"3. Explore results in RShiny app","text":"<p>In a separate browser window, the RShiny app will deploy to provide an interface where you can interact with the data.</p> <p>You will be able to see information about your sample data, perform QC/exploratory analysis, and take a look at results of differential expression analyses.</p> Click to show animation <p></p>"},{"location":"011_tertiary_analysis_data_studios/#explore-sample-clustering-with-pca","title":"Explore sample clustering with PCA","text":"<p>Under the 'QC/Exploratory' tab, select the PCA (Principal Component Analysis) plot to visualize how the samples group together based on their gene expression profiles.</p> <p>In this example, we used RNA-seq data from the publicly available ENCODE project, which includes samples from four different cell lines: GM12878 (a lymphoblastoid cell line), K562 (a chronic myelogenous leukemia cell line), MCF-7 (a breast cancer cell line), and H1-hESC (human embryonic stem cells).</p> <p>Here\u2019s what to look for in the PCA plot:</p> <ul> <li> <p>Replicate clustering: Ideally, replicates of the same cell type should cluster closely together. For example, you should see the replicates of MCF-7 (breast cancer cell line) grouping together. This indicates consistent gene expression profiles among replicates.</p> </li> <li> <p>Cell type separation: Different cell types should form distinct clusters. For instance, GM12878, K562, MCF-7, and H1-hESC samples should each form their own separate clusters, reflecting their unique gene expression patterns.</p> </li> </ul> <p>Using this PCA plot, you can gain insights into the consistency and quality of your RNA-seq data, identify any potential issues, and understand the major sources of variation among your samples - directly in Platform.</p> Click to show animation <p></p>"},{"location":"011_tertiary_analysis_data_studios/#explore-gene-expression-changes-with-volcano-plots","title":"Explore gene expression changes with Volcano plots","text":"<p>Under the Differential tab, select Volcano plots to compare genes with significant changes in expression between two samples. For example, you can filter for 'Type: H1 vs MCF-7' to view the differences in expression between these two cell lines.</p> <ol> <li> <p>Identify upregulated and downregulated genes: The x-axis of the volcano plot represents the log2 fold change in gene expression between the H1 and MCF-7 samples, while the y-axis represents the statistical significance of the changes.</p> <ul> <li>Upregulated genes in MCF-7: Genes on the left side of the plot (negative fold change) are upregulated in the MCF-7 samples compared to H1. For example, you may notice the SHH gene, which is known to be upregulated in cancer cell lines, prominently appearing here.</li> </ul> </li> <li> <p>Filtering for specific genes: If you are interested in specific genes, use the filter function. For example, you can filter for the SHH gene in the table below the plot. This will allow you to quickly locate and examine this gene in more detail.</p> </li> <li> <p>Gene expression bar plot: After filtering for the SHH gene, select it to navigate to a gene expression bar plot. This plot will show you the expression levels of SHH across all samples, allowing you to see in which samples it is most highly expressed.</p> <ul> <li>In this case, you will see that SHH is most highly expressed in MCF-7, which aligns with its known role in cancer cell proliferation.</li> </ul> </li> </ol> <p>Using the volcano plot, you can effectively identify and explore the genes with the most significant and meaningful changes in expression between your samples, providing a deeper understanding of the molecular differences.</p> Click to show animation <p></p>"},{"location":"011_tertiary_analysis_data_studios/#4-collaborate-in-the-data-studio","title":"4. Collaborate in the data studio","text":"<p>To share the results of your RNAseq analysis or allow colleagues to perform exploratory analysis as we just did, you can share a link to the data studio by selecting the three dots next to the status message for the data studio you want to share, then select Copy data studio URL. Using this link, other authenticated users with the \"Connect\" role (at minimum) can access the session directly.</p>"},{"location":"011_tertiary_analysis_data_studios/#5-takeaway","title":"5. Takeaway","text":"<p>This example demonstrates how Data Studios allows you to perform interactive analysis and explore the results of your secondary data analysis all within one unified platform. It simplifies the setup and data management process, making it easier for you to gain insights from your data efficiently.</p>"},{"location":"012_setting_up_data_studio/","title":"Set up a Data Studio","text":""},{"location":"012_setting_up_data_studio/#data-studio-setup","title":"Data Studio setup","text":""},{"location":"012_setting_up_data_studio/#create-a-data-studio","title":"Create a data studio","text":""},{"location":"012_setting_up_data_studio/#hidden-heading","title":"1. Add a data studio","text":"<p>To create a data studio, select Add data studio and select a template. Currently, templates for Jupyter, VS Code, and RStudio are available.</p> Click to show animation <p></p>"},{"location":"012_setting_up_data_studio/#hidden-heading","title":"2. Select a compute environment","text":"<p>Currently, only AWS Batch is supported.</p>"},{"location":"012_setting_up_data_studio/#hidden-heading","title":"3. Mount data using Data Explorer","text":""},{"location":"012_setting_up_data_studio/#create-a-data-link","title":"Create a data link","text":"<p>To enable access to data in a dtudio, create a custom data link pointing to the directory in the AWS S3 bucket where the results are saved. This will allow us to read and write only the data we need from cloud storage, from within our Studio.</p> <p>Select the Add cloud bucket button in Data Explorer and specify the path to the output directory:</p> <p></p>"},{"location":"012_setting_up_data_studio/#mount-the-data-link-into-the-studio","title":"Mount the data link into the studio","text":"<p>Select data to mount into your data studio environment using the Fusion file system in Data Explorer. In the Data Explorer, you can select the newly created data link to mount.</p> <p>This data will be available at <code>/workspace/data/&lt;dataset&gt;</code>.</p> Click to show animation <p></p>"},{"location":"012_setting_up_data_studio/#hidden-heading","title":"4. Resources for environment","text":"<p>Enter a CPU or memory allocation for your data studio environment (optional). The default is 2 CPUs and 8192 MB of memory.</p> <p>Then, select Add.</p> <p>The data studio environment will be available in the Data Studios landing page with the status 'stopped'. Select the three dots and Start to begin running the studio.</p> Click to show animation <p></p> <p></p>"},{"location":"012_setting_up_data_studio/#connect-to-a-data-studio","title":"Connect to a data studio","text":"<p>To connect to a running data studio session, select the three dots next to the status message and choose Connect. A new browser tab will open, displaying the status of the data studio session. Select Connect. </p>"},{"location":"012_setting_up_data_studio/#collaborate-in-a-data-studio","title":"Collaborate in a data studio","text":"<p>Collaborators can also join a data studios session in your workspace. For example, to share the results of the nf-core/rnaseq pipeline, you can share a link by selecting the three dots next to the status message for the data studio you want to share, then select Copy data studio URL. Using this link, other authenticated users with the \"Connect\" role (at minimum) can access the session directly.</p> <p></p>"},{"location":"012_setting_up_data_studio/#stop-a-data-studio","title":"Stop a data studio","text":"<p>To stop a running session, select the three dots next to the status and select Stop. Any unsaved analyses or results will be lost.</p> <p></p> Advanced <p>For a more detailed use case of performing tertiary analysis with the results of the nf-core/rnaseq pipeline in an RStudio/RShiny app environment, take see Tertiary analysis with Data Studios.</p>"},{"location":"012_setting_up_data_studio/#checkpoints-in-data-studios","title":"Checkpoints in Data Studios","text":"<p>When starting a data studio, a checkpoint gets created. This checkpoint allows you to restart a data studio with previously installed software and changes made to the root filesystem of the container. Please note, that if you stop a data studio and restart it, this will restart it from the latest checkpoint. To go back to a specific previous configuration of data studio session, please restart it from a checkpoint as highlighted in the screenshot below:</p> <p></p>"},{"location":"012_setting_up_data_studio/#more-information","title":"More information","text":"<p>For a detailed explanation about specific concepts of Data Studios and the tools preinstalled in Data Studios images, see the Seqera Platform docs.</p> Advanced <p>For additional details on Data Studios based on a demonstration from Rob Newman, see Data Studios deep dive.  </p>"},{"location":"013_data_studios_deep_dive/","title":"Data Studios deep dive","text":"<p>This content is transcribed from a Data Studios demo presented by Rob Newman. </p>"},{"location":"013_data_studios_deep_dive/#data-storage-and-data-links","title":"Data storage and data links","text":"<ol> <li> <p>Create a custom data link:</p> <ul> <li>Use the Data Explorer to add a specific data directory.</li> <li>Select Add cloud bucket and specify the exact path to your data.</li> <li>Note: Any data link added to a data studio session is read/write.</li> </ul> </li> <li> <p>Directory isolation:</p> <ul> <li>Once a directory is mounted to a data studio session, it cannot be accessed outside of that session.</li> <li>This isolation prevents others from overwriting your results.</li> <li>This is important ensure that only designated scientists or bioinformaticians can work with specific project directories.</li> </ul> </li> <li> <p>Fusion symlinks limitation:</p> <ul> <li>Fusion symlinks will not work outside the specified directory.</li> </ul> </li> <li> <p>Allowed buckets:</p> <ul> <li>Ensure the buckets you want to access through Data Studios are listed in the Allowed buckets section in the compute environment (CE).</li> </ul> </li> </ol>"},{"location":"013_data_studios_deep_dive/#compute-environment-resources","title":"Compute environment resources","text":"<ol> <li> <p>Resource management:</p> <ul> <li>When you use a compute environment (CE) for both pipelines and Data Studios sessions, they will compete for resources.</li> <li>To avoid stalling or losing work due to lack of CPU or memory, consider using a separate CE for Data Studios sessions.</li> </ul> </li> <li> <p>Large file handling:</p> <ul> <li>Staging large files (such as BAM files) can crash your session if there aren't enough resources.</li> <li>Ensure your data studio has adequate resources before working with large datasets.</li> </ul> </li> <li> <p>Identifying data studio sessions:</p> <ul> <li>Each studio session has a name that can be identified in AWS Batch within the appropriate CE as the running job name.</li> </ul> </li> </ol>"},{"location":"013_data_studios_deep_dive/#data-studios-infrastructure-and-mechanics","title":"Data Studios infrastructure and mechanics","text":"<ol> <li> <p>Container Web Server:</p> <ul> <li>Each Data Studio container includes a web server called Tower Connect, which communicates telemetry data to and from Platform.</li> <li>Custom containers may add a layer that includes this web server.</li> </ul> </li> <li> <p>Snapshots:</p> <ul> <li>Snapshots are created when you first create a session and each time you stop a studio.</li> <li>These snapshots help in saving and restoring your work environment.</li> </ul> </li> </ol>"},{"location":"013_data_studios_deep_dive/#collaboration","title":"Collaboration","text":"<ol> <li> <p>Monitoring activity:</p> <ul> <li>If a user is inactive for more than 5 minutes, their icon will disappear from the monitoring page, indicating they are not currently collaborating.</li> </ul> </li> <li> <p>Real-time collaboration:</p> <ul> <li>Only Jupyter and VS Code support real-time collaboration.</li> <li>RStudio requires a Pro license for real-time collaboration \u2014 discussions with Posit for this capability are ongoing.</li> </ul> </li> </ol>"},{"location":"014_automation_on_the_seqera_platform/","title":"Automation on the Seqera Platform","text":"<p>Seqera Platform provides multiple methods of programmatic interaction allowing you to automate the execution of pipelines, chain pipelines together, and integrate the Platform into third-party services of your choosing.</p>"},{"location":"014_automation_on_the_seqera_platform/#1-seqera-platform-api","title":"1. Seqera Platform API","text":"<p>The Seqera Platform public API is the lowest-level method of programmatic interaction. All operations available in the user interface can be achieved through the API. </p> <p>The API can be used to trigger the launch of pipelines based on a file event (such as the upload of a file to a bucket) or completion of a previous run.</p> <p>The API can be accessed from <code>https://api.cloud.seqera.io</code>.</p> <p>The full list of endpoints is available in Seqera's OpenAPI schema found here. The API requires an authentication token to be specified in every API request. This can be created in your user menu under Your tokens.</p> Click to show animation <p></p> <p>The token is only displayed once. Store your token in a secure place. Use this token to authenticate requests to the API.</p> Advanced <p>For an example of how to use the API to launch a pipeline, we can make the following request using cURL: </p> <pre><code>curl -X POST \"https://api.cloud.seqera.io/workflow/launch?workspaceId=38659136604200\" \\\n    -H \"Accept: application/json\" \\\n    -H \"Authorization: Bearer &lt;your_access_token&gt;\" \\\n    -H \"Content-Type: application/json\" \\\n    -H \"Accept-Version:1\" \\\n    -d '{\n    \"launch\": {\n        \"computeEnvId\": \"hjE97A8TvD9PklUb0hwEJ\",\n        \"runName\": \"first-time-pipeline-api-byname\",\n        \"pipeline\": \"first-time-pipeline\",\n        \"workDir\": \"s3://nf-ireland\",\n        \"revision\": \"master\"\n    }\n}'\n</code></pre>"},{"location":"014_automation_on_the_seqera_platform/#2-seqera-platform-cli","title":"2. Seqera Platform CLI","text":"<p>For bioinformaticians and scientists more comfortable with the CLI, Seqera Platform also comes with a command line utility called <code>tw</code> to manage resources. </p> <p>The CLI provides an interface to launch pipelines, manage compute environments, retrieve run metadata, and monitor runs on Platform. It provides a Nextflow-like experience for bioinformaticians who prefer the CLI and allows you store Seqera resource configuration (pipelines, compute environments, etc.) as code. The CLI is built on top of the Seqera Platform API but is simpler to use. For example, you can refer to resources by name instead of their unique identifier.</p> <p></p> <p>See the CLI GitHub repository for installation and usage details.</p> Advanced <p>For example, to launch the hello pipeline using the CLI:</p> <pre><code>tw launch hello --workspace seqeralabs/showcase\n</code></pre>"},{"location":"014_automation_on_the_seqera_platform/#3-seqerakit","title":"3. seqerakit","text":"<p><code>seqerakit</code> is a Python wrapper for the Seqera Platform CLI which can be leveraged to automate the creation of all of the entities in Seqera Platform via a YAML format configuration file. It can be used to automate the creation of entities, from organizations and workspaces to pipelines and compute environments, and the execution of workflows in one YAML.</p> <p>The key features are:</p> <ul> <li>Simple configuration: All of the command-line options available in the Seqera Platform CLI can be defined in simple YAML format.</li> <li>Infrastructure as Code: Enable users to manage and provision their infrastructure specifications.</li> <li>Automation: End-to-end creation of entities within Seqera Platform, from adding an organization to launching pipeline(s) within that organization.</li> </ul> <p>See the seqerakit GitHub repository for installation and usage details.</p> Advanced <p>For example, to launch the hello pipeline using seqerakit, you can create a YAML file called <code>hello.yaml</code> as follows:</p> <pre><code>launch:\n- name: \"hello-world\"\n    url: \"https://github.com/nextflow-io/hello\"\n    workspace: \"seqeralabs/showcase\"\n</code></pre> <p>Then run seqerakit with:</p> <pre><code>$ seqerakit hello.yaml\n</code></pre>"},{"location":"014_automation_on_the_seqera_platform/#resources","title":"Resources","text":"<p>Common use cases for the automation methods above include automatically executing a pipeline as data arrives from a sequencer, or integrating Seqera Platform into a broader customer facing application. For a step-by-step guide to set up these automation methods, see Workflow automation for Nextflow pipelines.</p> <p>For examples on how to use automation to chain workflows together, see Automating pipeline execution with Nextflow and Tower.</p>"},{"location":"015_seqera_pipelines/","title":"Seqera Pipelines","text":"<p>Seqera Pipelines is a list of the best open-source Nextflow workflows. Finding high quality pipelines is critical, so we\u2019ve created a tightly curated list of the very best workflows to begin with.</p> <p>Every pipeline comes with curated test data, so you can import into Seqera Platform and launch a test run in just a few clicks:</p> Click to show animation <p></p> <p>The pipeline details page provides key information about the workflow. With one click, you can add pipelines to your Seqera Platform Launchpad.</p> <p>Simply click the Launch button, specify whether you would like to add the pipeline to a Cloud or Enterprise instance, and provide your Platform user access token.</p> <p>You can then select the orgaization, workspace, and compute environment for the pipeline. </p> Click to show animation <p></p> <p>Select the Launch Pipeline tab to see various methods for launching the pipeline.</p> <p></p> <p>If you\u2019re more at home in the terminal, you can use the launch box to grab commands for Nextflow, Seqera Platform CLI, and nf-core/tools.</p>"},{"location":"016_seqera_containers/","title":"Seqera Containers","text":"<p>Containers have revolutionized research by providing portable environments that eliminate compatibility issues across different computing environments.</p> <p>Nextflow supports Docker containers, but pipeline developers often face challenges in having to write Dockerfile scripts for each workflow step.</p> <p>Projects like BioContainers offer pre-built images for Bioconda tools, but have limitations. Wave, our open-source on-demand container provisioning service, simplifies this process by allowing Nextflow developers to reference conda packages or a bundled Dockerfile to build containers on the fly.</p> <p>Seqera Containers enhance the Wave experience by allowing users to enter the names of their desired tools and instantly receive a container URI, usable for any purpose. The image is stored in a cache provided by AWS, ensuring reproducibility and availability for future runs without expiry.</p> <p>Users can:</p> <ol> <li> <p>Request any combination of packages</p> Click to show animation <p></p> </li> <li> <p>Select architecture and image format (such as linux/arm64 architecture)</p> Click to show animation <p></p> </li> <li> <p>Users can create Singularity images and download <code>.sif</code> files directly</p> Click to show animation <p></p> </li> </ol> <p>Select My Recent Containers to view containers you built previously. </p> <p>Select View build details for a container to view the full information of the Dockerfile, conda environment file, and build settings, as well as the complete build logs. Every container includes results from a security scan.</p> Click to show animation <p></p>"},{"location":"017_walkthrough_summary/","title":"One platform for the scientific data analysis life cycle","text":"<p>Throughout this guide, you have experienced how Seqera Platform streamlines the management, execution, monitoring, and analysis of Nextflow pipelines in the cloud. This centralized and intuitive interface offers numerous advantages:</p> <p> Ease of access: Enables all users to execute Nextflow pipelines with ease.</p> <p> Simplified cloud deployment: Allows for the deployment of pipelines on the cloud without the need to understand the underlying infrastructure.</p> <p> Real-time monitoring: Provides the ability to view the progress and outcomes of pipeline runs directly, bypassing the need for direct access to the execution environment.</p> <p> Enhanced provenance tracking: Facilitates the logging and tracking of pipeline provenance, enhancing reproducibility.</p> <p> Cloud data interaction: Supports seamless interaction with cloud-stored data, eliminating the need for direct cloud console or CLI interactions.</p> <p> Automated resource management: Reduces manual resource tuning, preventing allocation errors and optimizing task execution.</p> <p> Collaborative efficiency: Boosts productivity by enabling researchers to share, collaborate, and interpret results effortlessly, without additional infrastructure overhead.</p> <p>Seqera Platform empowers scientists to conduct high-throughput computing on a large scale, utilizing modern software engineering practices, all from a single, unified location. This guide has outlined how leveraging these capabilities can transform your research productivity and computational efficiency.</p>"},{"location":"018_resources/","title":"Resources","text":""},{"location":"018_resources/#hidden-heading","title":"Quick links","text":"<p> Seqera website</p> <p> Seqera Platform documentation</p> <p> Seqera Platform API</p> <p> Seqera Platform CLI</p> <p> seqerakit</p> <p> Nextflow documentation</p> <p> nf-core website</p>"},{"location":"018_resources/#hidden-heading","title":"Blog posts","text":"<p> Best practices for deploying pipelines with the Seqera Platform</p> <p> Breakthrough performance and cost-efficiency with the new Fusion file system</p> <p> Workflow automation for Nextflow pipelines</p>"}]}